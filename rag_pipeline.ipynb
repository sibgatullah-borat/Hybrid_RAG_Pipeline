{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SbnSuouNuq7",
        "outputId": "2f1d53df-edff-47dd-a6e2-635efd7f0b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (0.14.8)\n",
            "Requirement already satisfied: llama-index-llms-gemini in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.14.8)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.6.10)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-gemini) (0.8.5)\n",
            "Requirement already satisfied: pillow<11,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-gemini) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.36.0)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (5.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.12.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.26.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2.11.5)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.6.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (4.5.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.8->llama-index) (2.0.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.12.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.8.1)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.11.12)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=6.1.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.26.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (3.2.5)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-gemini llama-index-embeddings-huggingface transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Any\n",
        "import requests\n",
        "\n",
        "# Libraries for RAG components\n",
        "from llama_index.core import Document, VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# --- 0. Mock Data Setup (Simulating Kaggle/Public Data) ---\n",
        "\n",
        "# We simulate a corpus of financial reports/news articles, a great use case for RAG.\n",
        "# This makes the notebook immediately runnable without requiring file downloads.\n",
        "MOCK_DATA = [\n",
        "    {\"doc_id\": \"1\", \"text\": \"Google's Q3 2024 earnings report showed strong growth in Cloud services, which offset a slight dip in traditional Search ad revenue. AI integration was cited as a major driver across all segments.\"},\n",
        "    {\"doc_id\": \"2\", \"text\": \"The latest stock market volatility is primarily driven by interest rate decisions from the Federal Reserve. Analysts predict a continued focus on inflation control through Q1 2025.\"},\n",
        "    {\"doc_id\": \"3\", \"text\": \"Deep learning models, especially large language models (LLMs), require billions of parameters and vast datasets. The shift towards multimodal AI is the next major challenge in the field.\"},\n",
        "    {\"doc_id\": \"4\", \"text\": \"A key innovation in RAG is the use of hybrid search (dense + sparse) combined with cross-encoder reranking to improve the precision of context retrieved for the LLM.\"},\n",
        "    {\"doc_id\": \"5\", \"text\": \"Project Gemini, Google's next-generation AI model, is designed to be natively multimodal, capable of understanding and generating content across text, images, audio, and video.\"},\n",
        "    {\"doc_id\": \"6\", \"text\": \"The history of deep learning dates back to the perceptron models of the 1950s, but the modern revolution started with advancements in GPU processing and the Transformer architecture.\"},\n",
        "]\n",
        "\n",
        "# --- 1. Custom Retriever Classes (To match the user's snippet structure) ---\n",
        "\n",
        "class MockRetrievedDocument:\n",
        "    \"\"\"A lightweight class to simulate a document object returned from a retriever.\"\"\"\n",
        "    def __init__(self, text, score=0.0):\n",
        "        self.text = text\n",
        "        self.score = score\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.text == other.text\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.text)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MockDoc(Score: {self.score:.2f}, Text: '{self.text[:50]}...')\"\n",
        "\n",
        "\n",
        "class DenseRetriever(BaseRetriever):\n",
        "    \"\"\"\n",
        "    A semantic retriever using vector embeddings.\n",
        "    Simulates searching a vector store.\n",
        "    \"\"\"\n",
        "    def __init__(self, index: VectorStoreIndex):\n",
        "        self._index = index\n",
        "\n",
        "    async def _aretrieve(self, query: str, **kwargs) -> List[MockRetrievedDocument]:\n",
        "        # Use llama-index's internal retriever for semantic search\n",
        "        retriever = self._index.as_retriever(similarity_top_k=kwargs.get('k', 5))\n",
        "        nodes = retriever.retrieve(query)\n",
        "        # Convert to MockRetrievedDocument for consistent output\n",
        "        return [MockRetrievedDocument(n.text, n.score) for n in nodes]\n",
        "\n",
        "    def _retrieve(self, query: str, **kwargs) -> List[MockRetrievedDocument]:\n",
        "        return asyncio.run(self._aretrieve(query, **kwargs))\n",
        "\n",
        "\n",
        "class SparseRetriever:\n",
        "    \"\"\"\n",
        "    A keyword-based retriever (e.g., BM25).\n",
        "    Simulates searching a keyword/lexical index.\n",
        "    \"\"\"\n",
        "    def __init__(self, documents: List[str]):\n",
        "        # Use a simple split/filter for demonstration (actual BM25 would be used in production)\n",
        "        self.documents = documents\n",
        "\n",
        "    async def search(self, query: str, k: int) -> List[MockRetrievedDocument]:\n",
        "        # Simple token match simulation for speed in the demo\n",
        "        query_tokens = set(query.lower().split())\n",
        "        scored_docs = []\n",
        "        for doc_text in self.documents:\n",
        "            doc_tokens = set(doc_text.lower().split())\n",
        "            # Score by common words (simulates keyword relevance)\n",
        "            score = len(query_tokens.intersection(doc_tokens))\n",
        "            if score > 0:\n",
        "                scored_docs.append(MockRetrievedDocument(doc_text, score))\n",
        "\n",
        "        # Sort by score (descending) and return top k\n",
        "        scored_docs.sort(key=lambda x: x.score, reverse=True)\n",
        "        return scored_docs[:k]\n",
        "\n",
        "\n",
        "# --- 2. RAG Component Initialization ---\n",
        "\n",
        "# Initialize LLM (Gemini) - Assumes API Key is set in Colab secrets or environment\n",
        "llm = Gemini(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# Initialize Embedding Model (Free/Local) for Dense Retrieval\n",
        "# Uses a commonly used, small-footprint embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Load and Index Data\n",
        "print(\"--- Initializing Data and Indexes ---\")\n",
        "documents = [Document(text=d[\"text\"], id_=d[\"doc_id\"]) for d in MOCK_DATA]\n",
        "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "sparse_corpus = [d[\"text\"] for d in MOCK_DATA]\n",
        "\n",
        "# Initialize Retrievers\n",
        "dense_retriever = DenseRetriever(vector_index)\n",
        "sparse_retriever = SparseRetriever(sparse_corpus)\n",
        "\n",
        "# Initialize Reranker (Cross-Encoder)\n",
        "# This model is specifically trained for cross-document relevance scoring.\n",
        "# We will use the built-in LlamaIndex implementation.\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    top_n=3,\n",
        ")\n",
        "\n",
        "print(\"--- RAG Components Ready ---\")\n",
        "\n",
        "# --- 3. The Core Hybrid RAG Pipeline (Adapted from User's Snippet) ---\n",
        "\n",
        "async def hybrid_rag_pipeline(query: str):\n",
        "    \"\"\"\n",
        "    Executes the advanced hybrid RAG pipeline:\n",
        "    1. Parallel Retrieval (Dense + Sparse)\n",
        "    2. Merge results (Hybrid Search)\n",
        "    3. Rerank (Cross-Encoder)\n",
        "    4. LLM Generation (Streaming)\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running Hybrid RAG Pipeline for Query: '{query}' ---\")\n",
        "\n",
        "    # 1. Parallel Retrieval (Key Demonstrator of Latency Reduction)\n",
        "    print(\"1. Executing Dense (Semantic) and Sparse (Keyword) Retrieval in Parallel...\")\n",
        "    dense, sparse = await asyncio.gather(\n",
        "        dense_retriever._aretrieve(query, k=5),\n",
        "        sparse_retriever.search(query, k=5)\n",
        "    )\n",
        "\n",
        "    print(f\"   - Dense Results Found: {len(dense)}\")\n",
        "    print(f\"   - Sparse Results Found: {len(sparse)}\")\n",
        "\n",
        "    # 2. Merge (Hybrid Search for Maximizing Recall)\n",
        "    # Using 'set' to automatically deduplicate documents based on their text\n",
        "    combined = list(set(dense + sparse))\n",
        "    print(f\"2. Merged Results (Deduplicated): {len(combined)} documents.\")\n",
        "\n",
        "    # 3. Rerank (Cross-Encoder for Maximizing Precision)\n",
        "    # The reranker object expects llama_index Node/Document objects, so we mock this\n",
        "    # by using the text and passing it directly to the reranker's rank method.\n",
        "    # Note: In a real LlamaIndex setup, reranker.postprocess_nodes would be used.\n",
        "    print(\"3. Reranking combined documents using Cross-Encoder model...\")\n",
        "\n",
        "    # For the simplified demo, we use the internal `reranker.postprocess_nodes`\n",
        "    # which takes the initial retrieval list.\n",
        "    from llama_index.core.schema import NodeWithScore, TextNode\n",
        "\n",
        "    # Map the Mock Docs back to Llama Index Nodes for the reranker\n",
        "    nodes_to_rerank = [NodeWithScore(node=TextNode(text=doc.text), score=doc.score) for doc in combined]\n",
        "\n",
        "    # Perform reranking\n",
        "    reranked_nodes = reranker.postprocess_nodes(nodes_to_rerank, query_str=query)\n",
        "\n",
        "    # Extract the top context texts\n",
        "    top_docs = [node.text for node in reranked_nodes]\n",
        "    print(f\"   - Top 3 Contexts Selected after Reranking:\")\n",
        "    for i, doc_text in enumerate(top_docs):\n",
        "        print(f\"     [{i+1}] Score: {reranked_nodes[i].score:.4f} | Text: {doc_text[:60]}...\")\n",
        "\n",
        "    # 4. LLM with Streaming (Key Demonstrator of UX Optimization)\n",
        "    final_context = \"\\n\\n\".join(top_docs)\n",
        "    prompt = f\"Based ONLY on the following Context, answer the question accurately and concisely.\\n\\nContext:\\n{final_context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    print(\"\\n4. Generating Final Answer via Streaming LLM:\\n\")\n",
        "    print(\"--- LLM Output ---\")\n",
        "\n",
        "    full_response = \"\"\n",
        "    # Use the streaming API for reduced perceived latency\n",
        "    response_stream = llm.stream_complete(prompt=prompt)\n",
        "    for chunk in response_stream:\n",
        "        print(chunk.text, end=\"\", flush=True)\n",
        "        full_response += chunk.text\n",
        "\n",
        "    print(\"\\n--- End of Stream ---\")\n",
        "    return full_response\n",
        "\n",
        "\n",
        "# --- 4. Execution Block ---\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main execution function to run the RAG pipeline.\"\"\"\n",
        "    # Ensure GEMINI_API_KEY is available in the environment (e.g., Colab Secrets)\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: Please set the GEMINI_API_KEY environment variable (e.g., in Colab Secrets) to run the LLM step.\")\n",
        "        return\n",
        "\n",
        "    # Example questions that benefit from Hybrid Search\n",
        "    queries = [\n",
        "        \"What are the major drivers of Google's recent Q3 earnings, and what new AI model is they developing?\",\n",
        "        \"Explain why hybrid search is essential in modern RAG systems.\"\n",
        "    ]\n",
        "\n",
        "    for q in queries:\n",
        "        await hybrid_rag_pipeline(q)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In a Colab environment, use asyncio.run(main())\n",
        "    # but for a standard script, this works fine.\n",
        "    # If running in a Jupyter/Colab cell, use await main() instead of the block below.\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProgram interrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during execution: {e}\")\n",
        "\n",
        "# Note: In a real environment, you would use a requirements.txt file:\n",
        "# llama-index\n",
        "# llama-index-llms-gemini\n",
        "# llama-index-embeddings-huggingface\n",
        "# torch (for HuggingFace)\n",
        "# transformers (for HuggingFace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "b7pNBmYaOYo3",
        "outputId": "623ca3ca-4907-43b4-866c-12a52c3cc81c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2992725070.py:93: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Data and Indexes ---\n",
            "--- RAG Components Ready ---\n",
            "\n",
            "An error occurred during execution: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2992725070.py:213: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  print(f\"\\nAn error occurred during execution: {e}\")\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e6c697c",
        "outputId": "fb20d148-6efa-485b-f380-d8e3d213af64"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab secrets and set it as an environment variable\n",
        "# This ensures the 'llm = Gemini(...)' call can find it.\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "print(\"GEMINI_API_KEY has been set from Colab Secrets.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEMINI_API_KEY has been set from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "63ed6eec",
        "outputId": "5f6768f5-334a-4521-c058-500ef6d05a34"
      },
      "source": [
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Any\n",
        "import requests\n",
        "\n",
        "# Libraries for RAG components\n",
        "from llama_index.core import Document, VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# --- 0. Mock Data Setup (Simulating Kaggle/Public Data) ---\n",
        "\n",
        "# MOCK_DATA is already defined in a previous cell, ensuring it's available.\n",
        "\n",
        "# --- 1. Custom Retriever Classes ---\n",
        "\n",
        "class MockRetrievedDocument:\n",
        "    \"\"\"A lightweight class to simulate a document object returned from a retriever.\"\"\"\n",
        "    def __init__(self, text, score=0.0):\n",
        "        self.text = text\n",
        "        self.score = score\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.text == other.text\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.text)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MockDoc(Score: {self.score:.2f}, Text: '{self.text[:50]}...')\"\n",
        "\n",
        "\n",
        "class DenseRetriever(BaseRetriever):\n",
        "    \"\"\"\n",
        "    A semantic retriever using vector embeddings.\n",
        "    Simulates searching a vector store.\n",
        "    \"\"\"\n",
        "    def __init__(self, index: VectorStoreIndex):\n",
        "        self._index = index\n",
        "\n",
        "    async def _aretrieve(self, query: str, **kwargs) -> List[MockRetrievedDocument]:\n",
        "        # Use llama-index's internal retriever for semantic search\n",
        "        retriever = self._index.as_retriever(similarity_top_k=kwargs.get('k', 5))\n",
        "        nodes = retriever.retrieve(query)\n",
        "        # Convert to MockRetrievedDocument for consistent output\n",
        "        return [MockRetrievedDocument(n.text, n.score) for n in nodes]\n",
        "\n",
        "    def _retrieve(self, query: str, **kwargs) -> List[MockRetrievedDocument]:\n",
        "        return asyncio.run(self._aretrieve(query, **kwargs))\n",
        "\n",
        "\n",
        "class SparseRetriever:\n",
        "    \"\"\"\n",
        "    A keyword-based retriever (e.g., BM25).\n",
        "    Simulates searching a keyword/lexical index.\n",
        "    \"\"\"\n",
        "    def __init__(self, documents: List[str]):\n",
        "        # Use a simple split/filter for demonstration (actual BM25 would be used in production)\n",
        "        self.documents = documents\n",
        "\n",
        "    async def search(self, query: str, k: int) -> List[MockRetrievedDocument]:\n",
        "        # Simple token match simulation for speed in the demo\n",
        "        query_tokens = set(query.lower().split())\n",
        "        scored_docs = []\n",
        "        for doc_text in self.documents:\n",
        "            doc_tokens = set(doc_text.lower().split())\n",
        "            # Score by common words (simulates keyword relevance)\n",
        "            score = len(query_tokens.intersection(doc_tokens))\n",
        "            if score > 0:\n",
        "                scored_docs.append(MockRetrievedDocument(doc_text, score))\n",
        "\n",
        "        # Sort by score (descending) and return top k\n",
        "        scored_docs.sort(key=lambda x: x.score, reverse=True)\n",
        "        return scored_docs[:k]\n",
        "\n",
        "\n",
        "# --- 3. The Core Hybrid RAG Pipeline (Adapted from User's Snippet) ---\n",
        "\n",
        "async def hybrid_rag_pipeline(query: str):\n",
        "    \"\"\"\n",
        "    Executes the advanced hybrid RAG pipeline:\n",
        "    1. Parallel Retrieval (Dense + Sparse)\n",
        "    2. Merge results (Hybrid Search)\n",
        "    3. Rerank (Cross-Encoder)\n",
        "    4. LLM Generation (Streaming)\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running Hybrid RAG Pipeline for Query: '{query}' ---\")\n",
        "\n",
        "    # 1. Parallel Retrieval (Key Demonstrator of Latency Reduction)\n",
        "    print(\"1. Executing Dense (Semantic) and Sparse (Keyword) Retrieval in Parallel...\")\n",
        "    dense, sparse = await asyncio.gather(\n",
        "        dense_retriever._aretrieve(query, k=5),\n",
        "        sparse_retriever.search(query, k=5)\n",
        "    )\n",
        "\n",
        "    print(f\"   - Dense Results Found: {len(dense)}\")\n",
        "    print(f\"   - Sparse Results Found: {len(sparse)}\")\n",
        "\n",
        "    # 2. Merge (Hybrid Search for Maximizing Recall)\n",
        "    # Using 'set' to automatically deduplicate documents based on their text\n",
        "    combined = list(set(dense + sparse))\n",
        "    print(f\"2. Merged Results (Deduplicated): {len(combined)} documents.\")\n",
        "\n",
        "    # 3. Rerank (Cross-Encoder for Maximizing Precision)\n",
        "    # The reranker object expects llama_index Node/Document objects, so we mock this\n",
        "    # by using the text and passing it directly to the reranker's rank method.\n",
        "    # Note: In a real LlamaIndex setup, reranker.postprocess_nodes would be used.\n",
        "    print(\"3. Reranking combined documents using Cross-Encoder model...\")\n",
        "\n",
        "    from llama_index.core.schema import NodeWithScore, TextNode\n",
        "\n",
        "    # Map the Mock Docs back to Llama Index Nodes for the reranker\n",
        "    nodes_to_rerank = [NodeWithScore(node=TextNode(text=doc.text), score=doc.score) for doc in combined]\n",
        "\n",
        "    # Perform reranking\n",
        "    reranked_nodes = reranker.postprocess_nodes(nodes_to_rerank, query_str=query)\n",
        "\n",
        "    # Extract the top context texts\n",
        "    top_docs = [node.text for node in reranked_nodes]\n",
        "    print(f\"   - Top 3 Contexts Selected after Reranking:\")\n",
        "    for i, doc_text in enumerate(top_docs):\n",
        "        print(f\"     [{i+1}] Score: {reranked_nodes[i].score:.4f} | Text: {doc_text[:60]}...\")\n",
        "\n",
        "    # 4. LLM with Streaming (Key Demonstrator of UX Optimization)\n",
        "    final_context = \"\\n\\n\".join(top_docs)\n",
        "    prompt = f\"Based ONLY on the following Context, answer the question accurately and concisely.\\n\\nContext:\\n{final_context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    print(\"\\n4. Generating Final Answer via Streaming LLM:\\n\")\n",
        "    print(\"--- LLM Output ---\")\n",
        "\n",
        "    full_response = \"\"\n",
        "    # Use the streaming API for reduced perceived latency\n",
        "    response_stream = llm.stream_complete(prompt=prompt)\n",
        "    for chunk in response_stream:\n",
        "        print(chunk.text, end=\"\", flush=True)\n",
        "        full_response += chunk.text\n",
        "\n",
        "    print(\"\\n--- End of Stream ---\")\n",
        "    return full_response\n",
        "\n",
        "\n",
        "# --- 2. RAG Component Initialization (Now after definitions) ---\n",
        "\n",
        "# Re-initialize LLM (Gemini)\n",
        "# This code is copied from cell b7pNBmYaOYo3, but will now succeed after the API key is set.\n",
        "llm = Gemini(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# Initialize Embedding Model (Free/Local) for Dense Retrieval\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Load and Index Data (MOCK_DATA is already defined in a previous cell)\n",
        "documents = [Document(text=d[\"text\"], id_=d[\"doc_id\"]) for d in MOCK_DATA]\n",
        "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "sparse_corpus = [d[\"text\"] for d in MOCK_DATA]\n",
        "\n",
        "# Initialize Retrievers (DenseRetriever and SparseRetriever are now defined)\n",
        "dense_retriever = DenseRetriever(vector_index)\n",
        "sparse_retriever = SparseRetriever(sparse_corpus)\n",
        "\n",
        "# Initialize Reranker (Cross-Encoder)\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    top_n=3,\n",
        ")\n",
        "\n",
        "print(\"--- RAG Components Re-Initialized Successfully ---\")\n",
        "\n",
        "# Now, run the pipeline with your query:\n",
        "my_query = \"What were the key financial results for Google in Q3 2024?\"\n",
        "await hybrid_rag_pipeline(my_query)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2572577412.py:150: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RAG Components Re-Initialized Successfully ---\n",
            "\n",
            "--- Running Hybrid RAG Pipeline for Query: 'What were the key financial results for Google in Q3 2024?' ---\n",
            "1. Executing Dense (Semantic) and Sparse (Keyword) Retrieval in Parallel...\n",
            "   - Dense Results Found: 5\n",
            "   - Sparse Results Found: 5\n",
            "2. Merged Results (Deduplicated): 6 documents.\n",
            "3. Reranking combined documents using Cross-Encoder model...\n",
            "   - Top 3 Contexts Selected after Reranking:\n",
            "     [1] Score: 5.3227 | Text: Google's Q3 2024 earnings report showed strong growth in Clo...\n",
            "     [2] Score: -7.2088 | Text: The latest stock market volatility is primarily driven by in...\n",
            "     [3] Score: -11.2541 | Text: Project Gemini, Google's next-generation AI model, is design...\n",
            "\n",
            "4. Generating Final Answer via Streaming LLM:\n",
            "\n",
            "--- LLM Output ---\n",
            "Google's Q3 2024 earnings report showed strong growth in Cloud services, which offset a slight dip in traditional Search ad revenue.\n",
            "--- End of Stream ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Google's Q3 2024 earnings report showed strong growth in Cloud services, which offset a slight dip in traditional Search ad revenue.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}